import os
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

# ğŸ“Œ Configuration des paramÃ¨tres par dÃ©faut
default_args = {
    "owner": "etienneeskinazi",
    "depends_on_past": False,
    "start_date": datetime(2025, 2, 18),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

# ğŸ“‚ RÃ©cupÃ©ration des chemins
dag_folder = os.path.dirname(os.path.realpath(__file__))
project_root = os.path.abspath(os.path.join(dag_folder, ".."))

# ğŸ“Œ Chemin du script Bash de mise Ã  jour
update_script = os.path.join(project_root, "dbt_transform_data_scripts", "update_dbt_joined_file.sh")

# ğŸ“Œ DÃ©finition du DAG
with DAG(
    dag_id="dag_update_dbt_pipeline",
    default_args=default_args,
    schedule_interval=None,  # ExÃ©cution manuelle ou planifiÃ©e via lâ€™UI Airflow
    catchup=False,
    max_active_runs=1,
) as dag:

    # ğŸ“Œ TÃ¢che pour exÃ©cuter le script Bash
    run_update_script = BashOperator(
        task_id="run_update_dbt",
        bash_command=f"bash {update_script} {project_root}",
    )

    run_update_script
